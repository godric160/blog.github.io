<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Godric160个人博客</title>
  
  <subtitle>Mens et Manus</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://godric160.com/"/>
  <updated>2018-02-13T07:33:21.550Z</updated>
  <id>https://godric160.com/</id>
  
  <author>
    <name>Godric160</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Machine Learning--Linear Regression</title>
    <link href="https://godric160.com/2018/02/12/Machine_Learning_episode_02/"/>
    <id>https://godric160.com/2018/02/12/Machine_Learning_episode_02/</id>
    <published>2018-02-12T10:50:04.000Z</published>
    <updated>2018-02-13T07:33:21.550Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><p><strong>Notation</strong>：  </p><ol><li>$x_{i}$ denote the “input” variables,also called input features;  </li><li>$y_{i}$  denote the “output” or target variable that we are trying to predict;  </li><li>$(x^{(i)},y^{(i)})$  is called a training example, and the dataset that we’ll be using to learn—a list of m training examples ${(x^{(i)} , y^{(i)} ); i = 1, . . . , m}$ —is called a training set</li></ol><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s.</p><script type="math/tex; mode=display">J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}</script><p>This function is otherwise called the “Squared error function”, or “Mean squared error”.The mean is halved $\frac{1}{2}$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term.</p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p><img src="http://p194z0s8t.bkt.clouddn.com/gradient%20descent.png" alt="Gradient Descent"></p><p>The <u>slope</u> of the tangent is the derivative at that point and it will give us a <u>direction</u> to move towards.<br>The size of each step is determined by the parameter $\alpha$, which is called the <u>learning rate</u>.<br>The image above shows us two different starting points that end up in two different places.</p><p>The gradient descent algorithm is:<br>repeat until convergence:  </p><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad \text{(for j=0 and j=1)}</script><p><strong>Simultaneously update:</strong></p><script type="math/tex; mode=display">\begin{align*}& temp0:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \newline& temp1:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1}) \newline& \theta_{0}:=temp0 \newline& \theta_{1}:=temp1 \newline\end{align*}</script><p><strong>About parameter alpha</strong><br><img src="http://p194z0s8t.bkt.clouddn.com/gradient%20descent%20parameter%20alpha.png" alt="about parameter alpha"><br>We should adjust our parameter α to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p><p><img src="http://p194z0s8t.bkt.clouddn.com/gradient%20descent%20minimum.png" alt="gradient descent minimum">  </p><h3 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h3><p>The following is a derivation of $\frac{\partial}{\partial\theta_{j}}J(\theta)$ for example</p><script type="math/tex; mode=display">\begin{align}\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})& = \frac{\partial}{\partial\theta_{j}} \frac{1}{2m} \sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2} \\\& = \frac{\partial}{\partial\theta_{j}} \frac{1}{2m} \sum\limits_{i=1}^{m}(\theta_{0}+\theta_{1}x_{i}-y_{i})^{2}\end{align}</script><p>For $\theta_{0}$</p><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1})=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})</script><p>For $\theta_{1}$</p><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1})=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})\cdot x_{i}</script><p>Finally,we can substitute our actual cost function and our actual hypothesis function and modify the equation to :</p><script type="math/tex; mode=display">\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i}) \\\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}((h_{\theta}(x_{i})-y_{i})x_{i})</script><p>This method looks at every example in the entire training set on every step, and is called <u>batch gradient descent</u>.</p><h3 id="Multiple-Features"><a href="#Multiple-Features" class="headerlink" title="Multiple Features"></a>Multiple Features</h3><p>Notation:</p><script type="math/tex; mode=display">\begin{align*}x_j^{(i)} &= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}& = \text{the input (features) of the }i^{th}\text{ training example} \newlinem &= \text{the number of training examples} \newlinen &= \text{the number of features}\end{align*}</script><p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p><script type="math/tex; mode=display">h_{\theta} (x) = \theta_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + \theta_{3} x_{3} + \cdots + \theta_{n} x_{n}</script><p>Define: $x_0=1$</p><script type="math/tex; mode=display">h_{\theta} (x) = \theta_{0} x_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + \theta_{3} x_{3} + \cdots + \theta_{n} x_{n}</script><p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p><script type="math/tex; mode=display">\begin{align*}h_\theta(x)=\begin{bmatrix}\theta_{0} \hspace{2em} \theta_{1} \hspace{2em} \cdots \hspace{2em} \theta_{n}\end{bmatrix}\begin{bmatrix}x_{0} \newline x_{1} \newline \vdots \newline x_{n}\end{bmatrix}= \theta^{T}x\end{align*}</script><p>Gradient Descent</p><p>when $n&gt;1$</p><script type="math/tex; mode=display">\begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*}</script><h3 id="Gradient-Descent-Tricks"><a href="#Gradient-Descent-Tricks" class="headerlink" title="Gradient Descent Tricks"></a>Gradient Descent Tricks</h3><h4 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h4><p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p><p>Ideally:</p><script type="math/tex; mode=display">-1\leq x_{(i)}\leq 1</script><p>or</p><script type="math/tex; mode=display">-0.5\leq x_{(i)}\leq 0.5</script><p><strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.</p><p><strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.<br>To implement both of these techniques, adjust your input values as shown in this formula:</p><script type="math/tex; mode=display">x_i := \dfrac{x_i - \mu_i}{s_i}</script><p>Where $\mu_i$ is the average of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.</p><p>For example, if $x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then,</p><script type="math/tex; mode=display">x_i := \dfrac{price-1000}{1900}</script><h4 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h4><p><strong>Debugging gradient descent</strong>. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(θ)$ over the number of iterations of gradient descent. If $J(θ)$ ever increases, then you probably need to decrease $\alpha$.</p><p><strong>Automatic convergence test</strong>. Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it’s difficult to choose this threshold value.</p><p>It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.</p><p>Usually,choose a set of $\alpha$ for test,<br>like:0.001 , 0.003 , 0.01 , 0.03 , … 1  </p><p>For these different values of $\alpha$ are just plot $J(\theta)$ as a function of number of iterations, and then pick the value of $\alpha$ that seems to be causing $J(\theta)$ to decrease rapidly.</p><p><strong>To summarize:</strong></p><p>If $\alpha$ is too small: slow convergence.</p><p>If $\alpha$ is too large: ￼may not decrease on every iteration and thus may not converge.</p><h4 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h4><p>We can <strong>combine</strong> multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking x1⋅x2.</p><p>For example:<br>$area=width * height$</p><p><strong>Polynomial Regression</strong><br>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p><p>We can change the behavior or curve of our hypothesis function by making it a <u>quadratic</u>, <u>cubic</u> or <u>square root</u> function (or any other form).</p><p>If you choose your features this way then <strong>feature scaling</strong> becomes very important.</p><h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h3><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>Normal equation formula derivation</p><script type="math/tex; mode=display">\begin{align}Y &= X  \theta \\\X^T Y&=X^T X \theta \\\(X^T X)^{-1}X^T Y &=\theta\end{align}</script><p>Comparison</p><div class="table-container"><table><thead><tr><th>Gradient Descent</th><th>Normal Equation</th></tr></thead><tbody><tr><td>Need to choose alpha</td><td>No need to choose alpha</td></tr><tr><td>Needs many iterations</td><td>No need to iterate</td></tr><tr><td>$O(kn^2)$</td><td>$O(n^3)$, need to calculate inverse of $X^T X$</td></tr><tr><td>Works well when $n$ is large</td><td>Slow if $n$ is very large</td></tr></tbody></table></div><h3 id="内容参考"><a href="#内容参考" class="headerlink" title="内容参考"></a>内容参考</h3><p><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning by Andrew Ng</a><br><a href="https://zhuanlan.zhihu.com/p/22757336" target="_blank" rel="noopener">掰开揉碎推导Normal Equation</a>   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Model-Representation&quot;&gt;&lt;a href=&quot;#Model-Representation&quot; class=&quot;headerlink&quot; title=&quot;Model Representation&quot;&gt;&lt;/a&gt;Model Representation&lt;/h3&gt;&lt;
      
    
    </summary>
    
      <category term="machine learning" scheme="https://godric160.com/categories/machine-learning/"/>
    
    
      <category term="Personal notes" scheme="https://godric160.com/tags/Personal-notes/"/>
    
      <category term="machine learning" scheme="https://godric160.com/tags/machine-learning/"/>
    
      <category term="linear regression" scheme="https://godric160.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning--Introduction</title>
    <link href="https://godric160.com/2018/01/31/Machine_Learning_episode_01/"/>
    <id>https://godric160.com/2018/01/31/Machine_Learning_episode_01/</id>
    <published>2018-01-31T09:35:04.000Z</published>
    <updated>2018-02-13T07:27:36.970Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习的目标是使得学得的模型能很好地适用于“新样本”，这个能力，称为“泛化”(generalization)能力。</p><p>归纳(induction)：从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律，“从样例中学习”显然就是个归纳的过程；</p><p>演绎(deduction)：从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体情况，例如从一组公理和推理规则推导出与之相恰的定理；</p><p>关于机器学习有两个广为人知的定义，第一个来自Arthur Samuel，一个较老且不太正式的定义：<br><blockquote class="blockquote-center"><p>The field of study that gives computers the ability to learn without being explicitly programmed. </p></blockquote>还有一种则来自于Carnegie Mellon的教授Tom Mitchell：<blockquote class="blockquote-center"><p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. </p></blockquote></p><p><strong>以下棋为例：</strong><br>E=下很多局棋所得的经验<br>T=下棋这个挑战<br>P=这个程序下赢棋的概率  </p><p>根据训练数据是否拥有标记信息，学习任务可大致分为两大类：</p><ol><li>Supervised learning—监督学习(“right answer” given)</li><li>Unsupervised learning—非监督学习(no feedback)</li></ol><h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a <strong>relationship</strong> between the <u>input</u> and the <u>output</u>.</p><h5 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h5><p>We are trying to predict results within a <u>continuous</u> output, meaning that we are trying to map input variables to some continuous function</p><h5 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h5><p>We are instead trying to predict results in a <u>discrete</u> output. In other words, we are trying to map input variables into discrete categories</p><h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h3><p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.</p><h5 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h5><p>We can derive this structure by clustering the data based on relationships among the variables in the data.With unsupervised learning there is no feedback based on the prediction results.</p><h3 id="归纳偏好"><a href="#归纳偏好" class="headerlink" title="归纳偏好"></a>归纳偏好</h3><p>当需要找到一条穿过所有训练样本的曲线时，存在着许多条曲线与其一致，所有这里的学习算法必须有某种偏好，才能找到它认为“正确”的模型。</p><p>“奥卡姆剃刀”(Occam’s razor)是一种常用的，自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”，在这里就是选曲线中最平滑的那个。</p><p>根据“没有免费的午餐”定理(No Free Lunch Theorem)，无论学习算法$Q<em>{a}$多聪明，学习算法$Q</em>{b}$多笨拙，它们的期望值是相同的，也就是说脱离了具体问题谈“什么学习算法更好”是毫无意义的。</p><h3 id="內容參考"><a href="#內容參考" class="headerlink" title="內容參考"></a>內容參考</h3><p><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew Ng</a><br><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="noopener">机器学习</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习的目标是使得学得的模型能很好地适用于“新样本”，这个能力，称为“泛化”(generalization)能力。&lt;/p&gt;
&lt;p&gt;归纳(induction)：从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律，“从样例中学习”显然就
      
    
    </summary>
    
      <category term="machine learning" scheme="https://godric160.com/categories/machine-learning/"/>
    
    
      <category term="Personal notes" scheme="https://godric160.com/tags/Personal-notes/"/>
    
      <category term="machine learning" scheme="https://godric160.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>自然常数的相关了解</title>
    <link href="https://godric160.com/2018/01/07/Eulers-Number/"/>
    <id>https://godric160.com/2018/01/07/Eulers-Number/</id>
    <published>2018-01-07T03:05:04.000Z</published>
    <updated>2018-01-07T03:38:59.952Z</updated>
    
    <content type="html"><![CDATA[<p>在学习线性回归的时候发现自己并不能理解正态分布的方程式，尤其对自然常数exp感到陌生，遂查阅相关资料，了解了一下关于这个自然常数的前生今世。<br>自然常数exp是一个数学常量，是个无限不循环小数，约为2.71828，它的定义为：</p><script type="math/tex; mode=display">\exp=\sum_{n=0}^{+ \infty} \frac{1}{n!}=\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+\frac{1}{4!}\cdots</script><p>或者是：</p><script type="math/tex; mode=display">\exp=\lim_{n \rightarrow + \infty}(1+\frac{1}{n})^{n}</script><p>它有时候被称为欧拉数(Euler’s number),因为十八世纪初,数学大师Leonard Euler发现了这个自然常数。当时，Euler试图解决Jacob Bernoulli于半世纪前提出的银行利率问题：</p><p>假定你在银行存了一笔钱，银行每年给100%的利率，一年后，你会得到:</p><script type="math/tex; mode=display">(1+100\%)^1 = 2</script><p><img src="http://p194z0s8t.bkt.clouddn.com/growth_interest_1.png" alt=""></p><p>现在假设银行每六个月结算一次利息，但只能提供利率的一半，即50%。在这种情况下，一年后的收益为:  </p><script type="math/tex; mode=display">(1+50\%)^2=2.25</script><p><img src="http://p194z0s8t.bkt.clouddn.com/growth_interest_6_months.png" alt=""></p><p>而假设银行每四个月提供33.3%复利息:</p><script type="math/tex; mode=display">(1+33.3\%)^{3} \approx 2.37</script><p><img src="http://p194z0s8t.bkt.clouddn.com/growth_interest_4_months_compound.png" alt=""></p><p>甚至银行愿意每天都提供利息的话，一年的利息:</p><script type="math/tex; mode=display">(1+\frac{1}{365})^{365} \approx 2.71456748202</script><p>如果银行丧心病狂到愿意按秒给利息了，1年31536000秒，利滚利的余额:</p><script type="math/tex; mode=display">(1+\frac{1}{31536000})^{31536000} \approx 2.7182817813</script><p>到头来才发现，也没多挣几个钱，而这个数值却越来越接近exp，所以exp就像另外一个数学常量π一样，假定直径长为1，通过割圆术分割出多边形的边数越多，算出来的周长也越接近于π。</p><h3 id="微积分中的exp"><a href="#微积分中的exp" class="headerlink" title="微积分中的exp"></a>微积分中的exp</h3><p>微积分中的exp更为神奇，对下式f(x)进行微分，无论进行几次结果还仍然为f(x)，难怪数学系学生会用exp比喻坚定不移的爱情！</p><script type="math/tex; mode=display">f(x)=\exp^{x}</script><p>就是说f(x)这条曲线上任意点的斜率等于当前f(x)的值，如下图所示：<br><img src="http://p194z0s8t.bkt.clouddn.com/20170105113844197.jpg" alt=""></p><p>就是说f(x)是一个无法降维的指数函数，因为它的导数等于其自身。</p><h3 id="相关参考"><a href="#相关参考" class="headerlink" title="相关参考"></a>相关参考</h3><p><a href="https://en.wikipedia.org/wiki/E_(mathematical_constant" target="_blank" rel="noopener">Wikipedia:e (mathematical constant)</a>)<br><a href="https://www.zhihu.com/question/20296247/answer/29370489" target="_blank" rel="noopener">数学里的e为什么叫做自然底数？是不是自然界里什么东西恰好是e？</a><br><a href="https://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/" target="_blank" rel="noopener">An Intuitive Guide To Exponential Functions &amp; e</a><br><a href="http://book.douban.com/subject/20453160/" target="_blank" rel="noopener">《天才引导的历程》</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习线性回归的时候发现自己并不能理解正态分布的方程式，尤其对自然常数exp感到陌生，遂查阅相关资料，了解了一下关于这个自然常数的前生今世。&lt;br&gt;自然常数exp是一个数学常量，是个无限不循环小数，约为2.71828，它的定义为：&lt;/p&gt;
&lt;script type=&quot;mat
      
    
    </summary>
    
      <category term="math" scheme="https://godric160.com/categories/math/"/>
    
    
      <category term="mathematical constant" scheme="https://godric160.com/tags/mathematical-constant/"/>
    
      <category term="Euler&#39;s Number" scheme="https://godric160.com/tags/Euler-s-Number/"/>
    
      <category term="exp" scheme="https://godric160.com/tags/exp/"/>
    
  </entry>
  
  <entry>
    <title>LaTex 数学公式</title>
    <link href="https://godric160.com/2018/01/06/LaTex-Mathematics/"/>
    <id>https://godric160.com/2018/01/06/LaTex-Mathematics/</id>
    <published>2018-01-06T04:55:52.000Z</published>
    <updated>2018-02-13T07:14:01.942Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始用hexo写博客，markdown语言采用MathJax引擎来显示数学公式，采用的是LaTex的语法，不仅方便使用，且渲染出来的公式比Word上好看多了。本文是鄙人在学习使用中边学边整理而成，并会不断更新，主要内容参考于文章下方的参考网页。</p><h3 id="hexo渲染问题"><a href="#hexo渲染问题" class="headerlink" title="hexo渲染问题"></a>hexo渲染问题</h3><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，有时候会莫名其妙有些公式不能正确渲染，网上最常见的解决方案是卸载hexo-renderer-marked，改用hexo-renderer-pandoc，通过下列指令完成：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure></p><h3 id="通用语法"><a href="#通用语法" class="headerlink" title="通用语法"></a>通用语法</h3><p>LaTeX公式有两种调用形式，一种是行内显示公式用<code>$$...$$</code>显示，还有一种是单独一行显示，用<code>$$$...$$$</code>，效果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">公式1：$Mathematics1$</span><br><span class="line"></span><br><span class="line">公式2：$$ Mathematics2 $$</span><br></pre></td></tr></table></figure></p><p>显示效果：</p><blockquote><p>公式1：$Mathematics1$</p><p>公式2：<script type="math/tex">Mathematics2</script></p></blockquote><p>具体各种用法均总结于下列表格中：</p><h3 id="各种结构体"><a href="#各种结构体" class="headerlink" title="各种结构体"></a>各种结构体</h3><div class="table-container"><table><thead><tr><th>constructions</th><th>LaTex Code</th><th></th><th>constructions</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\widetilde{abc}</td><td>$\widetilde{abc}$</td><td></td><td>\widehat{abc}</td><td>$\widehat{abc}$</td></tr><tr><td>\overleftarrow{abc}</td><td>$\overleftarrow{abc}$</td><td></td><td>\overrightarrow{abc}</td><td>$\overrightarrow{abc}$</td></tr><tr><td>\overline{abc}</td><td>$\overline{abc}$</td><td></td><td>\underline{abc}</td><td>$\underline{abc}$</td></tr><tr><td>\overbrace{abc}</td><td>$\overbrace{abc}$</td><td></td><td>\underbrace{abc}</td><td>$\underbrace{abc}$</td></tr><tr><td>\sqrt{abc}</td><td>$\sqrt{abc}$</td><td></td><td>\sqrt[n]{abc}</td><td>$\sqrt[n]{abc}$</td></tr><tr><td>f’</td><td>$f’$</td><td></td><td>\frac{abc}{xyz}</td><td>$\frac{abc}{xyz}$</td></tr></tbody></table></div><h3 id="希腊符号-小"><a href="#希腊符号-小" class="headerlink" title="希腊符号(小)"></a>希腊符号(小)</h3><div class="table-container"><table><thead><tr><th>Greek letter</th><th>LaTex Code</th><th></th><th>Greek letter</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\alpha</td><td>$\alpha$</td><td></td><td>o</td><td>$ o$</td></tr><tr><td>\beta</td><td>$\beta$</td><td></td><td>\pi</td><td>$\pi$</td></tr><tr><td>\gamma</td><td>$\gamma$</td><td></td><td>\varpi</td><td>$\varpi$</td></tr><tr><td>\delta</td><td>$\delta$</td><td></td><td>\rho</td><td>$\rho$</td></tr><tr><td>\epsilon</td><td>$\epsilon$</td><td></td><td>\varrho</td><td>$\varrho$</td></tr><tr><td>\varepsilon</td><td>$\varepsilon$</td><td></td><td>\sigma</td><td>$\sigma$</td></tr><tr><td>\zeta</td><td>$\zeta$</td><td></td><td>\varsigma</td><td>$\varsigma$</td></tr><tr><td>\eta</td><td>$\eta$</td><td></td><td>\tau</td><td>$\tau$</td></tr><tr><td>\theta</td><td>$\theta$</td><td></td><td>\upsilon</td><td>$\upsilon$</td></tr><tr><td>\vartheta</td><td>$\vartheta$</td><td></td><td>\phi</td><td>$\phi$</td></tr><tr><td>\kappa</td><td>$\kappa$</td><td></td><td>\varphi</td><td>$\varphi$</td></tr><tr><td>\lambda</td><td>$\lambda$</td><td></td><td>\chi</td><td>$\chi$</td></tr><tr><td>\mu</td><td>$\mu$</td><td></td><td>\psi</td><td>$\psi$</td></tr><tr><td>\nu</td><td>$\nu$</td><td></td><td>\omega</td><td>$\omega$</td></tr><tr><td>\xi</td><td>$\xi$</td><td></td><td></td></tr></tbody></table></div><h3 id="希腊符号-大"><a href="#希腊符号-大" class="headerlink" title="希腊符号(大)"></a>希腊符号(大)</h3><div class="table-container"><table><thead><tr><th>Greek letter</th><th>LaTex Code</th><th></th><th>Greek letter</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\Gamma</td><td>$\Gamma$</td><td></td><td>\Sigma</td><td>$\Sigma$</td></tr><tr><td>\Delta</td><td>$\Delta$</td><td></td><td>\Upsilon</td><td>$\Upsilon$</td></tr><tr><td>\Theta</td><td>$\Theta$</td><td></td><td>\Phi</td><td>$\Phi$</td></tr><tr><td>\Lambda</td><td>$\Lambda$</td><td></td><td>\Psi</td><td>$\Psi$</td></tr><tr><td>\Xi</td><td>$\Xi$</td><td></td><td>\Omega</td><td>$\Omega$</td></tr><tr><td>\Pi</td><td>$\Pi$</td><td></td><td></td></tr></tbody></table></div><h3 id="常用二元运算符"><a href="#常用二元运算符" class="headerlink" title="常用二元运算符"></a>常用二元运算符</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\pm</td><td>$\pm$</td><td></td><td>\mp</td><td>$\mp$</td></tr><tr><td>\times</td><td>$\times$</td><td></td><td>\div</td><td>$\div$</td></tr><tr><td>\ast</td><td>$\ast$</td><td></td><td>\cdot</td><td>$\cdot$</td></tr><tr><td>\circ</td><td>$\circ$</td><td></td><td>\bullet</td><td>$\bullet$</td></tr><tr><td>+</td><td>$ +$</td><td></td><td>-</td><td>$ -$</td></tr><tr><td>\cap</td><td>$\cap$</td><td></td><td>\cup</td><td>$\cup$</td></tr><tr><td>\vee</td><td>$\vee$</td><td></td><td>\wedge</td><td>$\wedge$</td></tr></tbody></table></div><h3 id="常用关系运算符"><a href="#常用关系运算符" class="headerlink" title="常用关系运算符"></a>常用关系运算符</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>&lt;</td><td>$ &lt;$</td><td></td><td>&gt;</td><td>$ &gt;$</td></tr><tr><td>\ll</td><td>$\ll$</td><td></td><td>\gg</td><td>$\gg$</td></tr><tr><td>\leq</td><td>$\leq$</td><td></td><td>\geq</td><td>$\geq$</td></tr><tr><td>=</td><td>$ =$</td><td></td><td>\neq</td><td>$\neq$</td></tr><tr><td>\approx</td><td>$\approx$</td><td></td><td>\doteq</td><td>$\doteq$</td></tr><tr><td>\subset</td><td>$\subset$</td><td></td><td>\supset</td><td>$\supset$</td></tr><tr><td>\subseteq</td><td>$\subseteq$</td><td></td><td>\supseteq</td><td>$\supseteq$</td></tr><tr><td>\in</td><td>$\in$</td><td></td><td>\in</td><td>$\in$</td></tr><tr><td>\mid</td><td>$\mid$</td><td></td><td>\parallel</td><td>$\parallel$</td></tr></tbody></table></div><h3 id="大小可变符号"><a href="#大小可变符号" class="headerlink" title="大小可变符号"></a>大小可变符号</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\sum</td><td>$\sum$</td><td></td><td></td><td></td></tr><tr><td>\prod</td><td>$\prod$</td><td></td><td>\coprod</td><td>$\coprod$</td></tr><tr><td>\bigcap</td><td>$\bigcap$</td><td></td><td>\bigcup</td><td>$\bigcup$</td></tr><tr><td>\bigvee</td><td>$\bigvee$</td><td></td><td>\bigwedge</td><td>$\bigwedge$</td></tr><tr><td>\int</td><td>$\int$</td><td></td><td>\oint</td><td>$\oint$</td></tr><tr><td>\iint</td><td>$\iint$</td><td></td><td>\iiint</td><td>$\iiint$</td></tr></tbody></table></div><h3 id="Log-like-符号"><a href="#Log-like-符号" class="headerlink" title="Log-like 符号"></a>Log-like 符号</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\arccos</td><td>$\arccos$</td><td></td><td>\cos</td><td>$\cos$</td></tr><tr><td>\arcsin</td><td>$\arcsin$</td><td></td><td>\sin</td><td>$\sin$</td></tr><tr><td>\arctan</td><td>$\arctan$</td><td></td><td>\tan</td><td>$\tan$</td></tr><tr><td>\log</td><td>$\log$</td><td></td><td>\ln</td><td>$\ln$</td></tr><tr><td>\exp</td><td>$\exp$</td><td></td><td>\max</td><td>$\max$</td></tr><tr><td>\lim</td><td>$\lim$</td><td></td><td></td></tr></tbody></table></div><h3 id="数学模式的口音"><a href="#数学模式的口音" class="headerlink" title="数学模式的口音"></a>数学模式的口音</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\hat{a}</td><td>$\hat{a}$</td><td></td><td>\check{a}</td><td>$\check{a}$</td></tr><tr><td>\acute{a}</td><td>$\acute{a}$</td><td></td><td>\grave{a}</td><td>$\grave{a}$</td></tr><tr><td>\bar{a}</td><td>$\bar{a}$</td><td></td><td>\vec{a}</td><td>$\vec{a}$</td></tr><tr><td>\dot{a}</td><td>$\dot{a}$</td><td></td><td>\ddot{a}</td><td>$\ddot{a}$</td></tr><tr><td>\breve{a}</td><td>$\breve{a}$</td><td></td><td>\tilde{a}</td><td>$\tilde{a}$</td></tr></tbody></table></div><h3 id="各种符号"><a href="#各种符号" class="headerlink" title="各种符号"></a>各种符号</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\ldotp</td><td>$\ldotp$</td><td></td><td>\cdotp</td><td>$\cdotp$</td></tr><tr><td>\ldots</td><td>$\ldots$</td><td></td><td>\cdots</td><td>$\cdots$</td></tr><tr><td>\colon</td><td>$\colon$</td><td></td><td>\neg</td><td>$\neg$</td></tr><tr><td>\forall</td><td>$\forall$</td><td></td><td>\exists</td><td>$\exists$</td></tr><tr><td>\partial</td><td>$\partial$</td><td></td><td>\infty</td><td>$\infty$</td></tr><tr><td>\sharp</td><td>$\sharp$</td><td></td><td></td></tr></tbody></table></div><h3 id="箭头符号"><a href="#箭头符号" class="headerlink" title="箭头符号"></a>箭头符号</h3><div class="table-container"><table><thead><tr><th>Symbols</th><th>LaTex Code</th><th></th><th>Symbols</th><th>LaTex Code</th></tr></thead><tbody><tr><td>\leftarrow</td><td>$\leftarrow$</td><td></td><td>\rightarrow</td><td>$\rightarrow$</td></tr><tr><td>\longleftarrow</td><td>$\longleftarrow$</td><td></td><td>\longrightarrow</td><td>$\longrightarrow$</td></tr><tr><td>\Leftarrow</td><td>$\Leftarrow$</td><td></td><td>\Rightarrow</td><td>$\Rightarrow$</td></tr><tr><td>\Longleftarrow</td><td>$\Longleftarrow$</td><td></td><td>\Longrightarrow</td><td>$\Longrightarrow$</td></tr><tr><td>\leftrightarrow</td><td>$\leftrightarrow$</td><td></td><td>\longleftrightarrow</td><td>$\longleftrightarrow$</td></tr><tr><td>\Leftrightarrow</td><td>$\Leftrightarrow$</td><td></td><td>\Longleftrightarrow</td><td>$\Longleftrightarrow$</td></tr><tr><td>\uparrow</td><td>$\uparrow$</td><td></td><td>\downarrow</td><td>$\downarrow$</td></tr><tr><td>\Uparrow</td><td>$\Uparrow$</td><td></td><td>\Downarrow</td><td>$\Downarrow$</td></tr></tbody></table></div><h3 id="多行公式显示方法："><a href="#多行公式显示方法：" class="headerlink" title="多行公式显示方法："></a>多行公式显示方法：</h3><h5 id="多个方程式"><a href="#多个方程式" class="headerlink" title="多个方程式"></a>多个方程式</h5><!-- 方程式间使用``\\``来进行换行，如果要给方程式后面加上标签，使用``\qquad ()``标记,效果如下所示： --><p>方程式间使用<code>\\</code>来进行换行，实际使用中，需要进行转义，即使用<code>\\\</code>来进行换行，效果如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">f(x)=\exp^&#123;x&#125;\\\</span><br><span class="line">f(y)=\exp^&#123;y&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><p>显示效果：</p><script type="math/tex; mode=display">f(x)=\exp^{x}\\\f(y)=\exp^{y}</script><h5 id="数学运算拆解的式子于等号对齐"><a href="#数学运算拆解的式子于等号对齐" class="headerlink" title="数学运算拆解的式子于等号对齐"></a>数学运算拆解的式子于等号对齐</h5><p>使用了align指令，每个<code>=</code>前都加上<code>&amp;</code>,使用<code>\\\</code>进行换行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">\sqrt&#123;37&#125; &amp; = \sqrt&#123;\frac&#123;73^2-1&#125;&#123;12^2&#125;&#125; \\\</span><br><span class="line"> &amp; = \sqrt&#123;\frac&#123;73^2&#125;&#123;12^2&#125;\cdot\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\</span><br><span class="line"> &amp; = \sqrt&#123;\frac&#123;73^2&#125;&#123;12^2&#125;&#125;\sqrt&#123;\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\</span><br><span class="line"> &amp; = \frac&#123;73&#125;&#123;12&#125;\sqrt&#123;1 - \frac&#123;1&#125;&#123;73^2&#125;&#125; \\\</span><br><span class="line"> &amp; \approx \frac&#123;73&#125;&#123;12&#125;\left(1 - \frac&#123;1&#125;&#123;2\cdot73^2&#125;\right)</span><br><span class="line">\end&#123;align&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><p>显示结果</p><script type="math/tex; mode=display">\begin{align}\sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\\ & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\\ & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\\ & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\\ & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right)\end{align}</script><h5 id="矩阵与对齐"><a href="#矩阵与对齐" class="headerlink" title="矩阵与对齐"></a>矩阵与对齐</h5><p>使用了array指令，同样使用<code>\\\</code>进行换行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\mathbf&#123;X&#125; =</span><br><span class="line">\left(</span><br><span class="line">  \begin&#123;array&#125;</span><br><span class="line">  &#123;ccc&#125;</span><br><span class="line">x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\</span><br><span class="line">x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">  \right)</span><br><span class="line">$$</span><br></pre></td></tr></table></figure></p><p>显示结果</p><script type="math/tex; mode=display">\mathbf{X} =    \left(  \begin{array}  {ccc}    x\_{11} & x\_{12} & \ldots \\\    x\_{21} & x\_{22} & \ldots \\\    \vdots & \vdots & \ddots    \end{array}  \right)</script><h5 id="括号使用与对齐"><a href="#括号使用与对齐" class="headerlink" title="括号使用与对齐"></a>括号使用与对齐</h5><p>在LaTeX中使用<code>\left</code>和<code>\right</code>成对出现来表示一对括号，如果不使用该指令，直接用<code>()</code>也是可以的，只是括号大小不会自动渲染。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\left(1+\left(\frac &#123;1&#125;&#123;\exp^2&#125;\right)\right) \qquad 1+(\frac &#123;1&#125;&#123;\exp^2&#125;)$$</span><br></pre></td></tr></table></figure></p><p>上面代码渲染出的两种括号大小如下所示：</p><script type="math/tex; mode=display">1+\left(\frac {1}{\exp^2}\right) \qquad 1+(\frac {1}{\exp^2})</script><p>另，当因括号层次需要调节括号大小时，使用<code>\big</code>,<code>\Big</code>,<code>\bigg</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$$\big(\Big(\bigg(\Bigg($$</span><br><span class="line">$$\big[\Big[\bigg[\Bigg[$$</span><br></pre></td></tr></table></figure></p><p>效果如下所示：</p><script type="math/tex; mode=display">\big(\Big(\bigg(\Bigg(</script><script type="math/tex; mode=display">\big[\Big[\bigg[\Bigg[</script><h5 id="上下限位置问题"><a href="#上下限位置问题" class="headerlink" title="上下限位置问题"></a>上下限位置问题</h5><p>这里需要使用<code>\limits</code>指令，即在$\sum\prod\cdots$这类可以设定上下限的指令后面接上<code>\limits</code>，具体效果如下所示：<br>未使用<code>\limits</code>指令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R^&#123;2&#125;=1-\frac&#123;\sum_&#123;i=1&#125;^&#123;m&#125;(\hat&#123;y_&#123;i&#125;&#125;-y_&#123;i&#125;)&#125;&#123;\sum_&#123;i=1&#125;^&#123;m&#125;(y_&#123;i&#125;-\bar&#123;y_&#123;i&#125;&#125;)&#125;</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">R^{2}=1- \frac{ \sum_{i=1}^{m}(\hat{y_{i}}-y_{i})}{\sum_{i=1}^{m}(y_{i}-\bar{y_{i}})}</script><p>使用了<code>\limits</code>指令的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R^&#123;2&#125;=1-\frac&#123;\sum\limits_&#123;i=1&#125;^&#123;m&#125;(\hat&#123;y_&#123;i&#125;&#125;-y_&#123;i&#125;)&#125;&#123;\sum\limits_&#123;i=1&#125;^&#123;m&#125;(y_&#123;i&#125;-\bar&#123;y_&#123;i&#125;&#125;)&#125;</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">R^{2}=1-\frac{\sum\limits_{i=1}^{m}(\hat{y_{i}}-y_{i})}{\sum\limits_{i=1}^{m}(y_{i}-\bar{y_{i}})}</script><h5 id="输入文字"><a href="#输入文字" class="headerlink" title="输入文字"></a>输入文字</h5><p>有时候需要在公式中加入文字表述，使用到的是<code>\text{}</code>指令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\text&#123;爱因斯坦质能方程&#125;:E=mc^2</span><br></pre></td></tr></table></figure></p><p>显示效果：</p><script type="math/tex; mode=display">\text{爱因斯坦质能方程}:E=mc^2</script><p>参考网页：<br><a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" target="_blank" rel="noopener">MathJax basic tutorial and quick reference</a><br><a href="https://en.wikibooks.org/wiki/LaTeX/Mathematics" target="_blank" rel="noopener">维基教科书:LaTeX/数学公式</a><br><a href="http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html" target="_blank" rel="noopener">LaTeX Math Symbols</a><br><a href="http://www.ctex.org/documents/shredder/tex_frame.html" target="_blank" rel="noopener">怎样输入完美的TeX公式</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近开始用hexo写博客，markdown语言采用MathJax引擎来显示数学公式，采用的是LaTex的语法，不仅方便使用，且渲染出来的公式比Word上好看多了。本文是鄙人在学习使用中边学边整理而成，并会不断更新，主要内容参考于文章下方的参考网页。&lt;/p&gt;
&lt;h3 id=&quot;
      
    
    </summary>
    
      <category term="tools" scheme="https://godric160.com/categories/tools/"/>
    
    
      <category term="LaTex" scheme="https://godric160.com/tags/LaTex/"/>
    
      <category term="markdown" scheme="https://godric160.com/tags/markdown/"/>
    
      <category term="mathematics" scheme="https://godric160.com/tags/mathematics/"/>
    
  </entry>
  
  <entry>
    <title>JavaCross01</title>
    <link href="https://godric160.com/2018/01/03/Javacross-01/"/>
    <id>https://godric160.com/2018/01/03/Javacross-01/</id>
    <published>2018-01-03T06:57:57.000Z</published>
    <updated>2018-01-03T08:09:10.695Z</updated>
    
    <content type="html"><![CDATA[<p>本文内容对应《Head First Java》一书中第一章的字谜游戏，相关单词内容可参考网址：<a href="https://www.studystack.com/flashcard-1700664" target="_blank" rel="noopener">studystack</a></p><p><img src="http://p194z0s8t.bkt.clouddn.com/javacross_origin_01.png" alt=""></p><h4 id="横排提示"><a href="#横排提示" class="headerlink" title="横排提示"></a>横排提示</h4><blockquote><p>4.Command-line invoker<br>6.Back again?<br>8.Can’t go both ways<br>9.Acronym for your laptop’s power<br>12.number variable type<br>13.Acronym for a chip<br>14.Say something<br>18.Quite a crew of characters<br>19.Announce a new class or method<br>21.What’s a prompt good for?</p></blockquote><h4 id="竖排提示"><a href="#竖排提示" class="headerlink" title="竖排提示"></a>竖排提示</h4><blockquote><p>1.Not an integer (or <strong>_</strong> your boat)<br>2.Come back empty-handed<br>3.Open house<br>5.‘Things’ holders<br>7.Until attitudes improve<br>10.Source code consumer<br>11.Can’t pin it down<br>13.Dept. of LAN jockeys<br>15.Shocking modifier<br>16.Just gotta have one<br>17.How to get things done<br>20.Bytecode consumer</p></blockquote><hr><h2 id="答案见下"><a href="#答案见下" class="headerlink" title="答案见下"></a>答案见下</h2><div class="table-container"><table><thead><tr><th>index</th><th>hint</th><th>word</th><th>index</th><th>hint</th><th>word</th></tr></thead><tbody><tr><td>1</td><td>Not an integer (or <strong>_</strong> your boat)</td><td>float</td><td>12</td><td>number variable type</td><td>int</td></tr><tr><td>2</td><td>Come back empty-handed</td><td>void</td><td>13</td><td>Acronym for a chip</td><td>IC</td></tr><tr><td>3</td><td>Open house</td><td>public</td><td>14</td><td>Say something</td><td>SystemOutPrint</td></tr><tr><td>4</td><td>Command-line invoker</td><td>java</td><td>15</td><td>Shocking modifier</td><td>Static</td></tr><tr><td>5</td><td>‘Things’ holders</td><td>arrays</td><td>16</td><td>Just gotta have one</td><td>main</td></tr><tr><td>6</td><td>Back again?</td><td>loop</td><td>17</td><td>How to get things done</td><td>method</td></tr><tr><td>7</td><td>Until attitudes improve</td><td>while</td><td>18</td><td>Quite a crew of characters</td><td>string</td></tr><tr><td>8</td><td>Can’t go both ways</td><td>branch</td><td>19</td><td>Announce a new class or method</td><td>declare</td></tr><tr><td>9</td><td>Acronym for your laptop’s power</td><td>DC</td><td>20</td><td>Bytecode consumer</td><td>JVM</td></tr><tr><td>10</td><td>Source code consumer</td><td>compiler</td><td>21</td><td>What’s a prompt good for?</td><td>command</td></tr><tr><td>11</td><td>Can’t pin it down</td><td>variable</td><td></td><td></td></tr></tbody></table></div><p>书中答案截图如下所示：<br><img src="http://p194z0s8t.bkt.clouddn.com/javacross_answer_01.png" alt=""></p><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><p><strong>index:16</strong>   <em>Just gotta have one</em></p><blockquote><p>无论程序有多大（也可以说不管有多少类），一定都会有一个main()来作为程序的起点。</p><footer><strong>Head First Java</strong></footer></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文内容对应《Head First Java》一书中第一章的字谜游戏，相关单词内容可参考网址：&lt;a href=&quot;https://www.studystack.com/flashcard-1700664&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;stud
      
    
    </summary>
    
      <category term="java" scheme="https://godric160.com/categories/java/"/>
    
    
      <category term="head_first" scheme="https://godric160.com/tags/head-first/"/>
    
      <category term="java_cross" scheme="https://godric160.com/tags/java-cross/"/>
    
  </entry>
  
  <entry>
    <title>Intro</title>
    <link href="https://godric160.com/2017/12/20/Intro/"/>
    <id>https://godric160.com/2017/12/20/Intro/</id>
    <published>2017-12-20T07:48:39.000Z</published>
    <updated>2018-01-03T07:37:22.553Z</updated>
    
    <content type="html"><![CDATA[<h2 id="例行公事，秀个恩爱先"><a href="#例行公事，秀个恩爱先" class="headerlink" title="例行公事，秀个恩爱先"></a>例行公事，秀个恩爱先</h2><p><img src="http://p194z0s8t.bkt.clouddn.com/Selfie.jpg" alt="intro_selfie"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;例行公事，秀个恩爱先&quot;&gt;&lt;a href=&quot;#例行公事，秀个恩爱先&quot; class=&quot;headerlink&quot; title=&quot;例行公事，秀个恩爱先&quot;&gt;&lt;/a&gt;例行公事，秀个恩爱先&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;http://p194z0s8t.bkt.clouddn
      
    
    </summary>
    
      <category term="life" scheme="https://godric160.com/categories/life/"/>
    
    
      <category term="life" scheme="https://godric160.com/tags/life/"/>
    
      <category term="selfie" scheme="https://godric160.com/tags/selfie/"/>
    
  </entry>
  
</feed>
