<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Machine Learining--Linear Regression]]></title>
    <url>%2F2018%2F02%2F12%2FMachine_Learning_episode_02%2F</url>
    <content type="text"><![CDATA[Model RepresentationNotation： $x_{i}$ denote the “input” variables,also called input features; $y_{i}$ denote the “output” or target variable that we are trying to predict; $(x^{(i)},y^{(i)})$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples ${(x^{(i)} , y^{(i)} ); i = 1, . . . , m}$ —is called a training set $h_{\theta}$ 为以 $\theta$ 为参数的hypothesis function； 以一次方程为例 h_{\theta}(x_{i})=\theta_{0}+\theta_{1}x_{i}Cost FunctionWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s. J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2}This function is otherwise called the “Squared error function”, or “Mean squared error”.The mean is halved $\frac{1}{2}$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\frac{1}{2}$ term. Gradient Descent The slope of the tangent is the derivative at that point and it will give us a direction to move towards.The size of each step is determined by the parameter $\alpha$, which is called the learning rate.The image above shows us two different starting points that end up in two different places. The gradient descent algorithm is:repeat until convergence: \theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad \text{(for j=0 and j=1)}Simultaneously update:$temp0:=\theta{0}-\alpha\frac{\partial}{\partial\theta{0}}J(\theta{0},\theta{1})$$temp1:=\theta{1}-\alpha\frac{\partial}{\partial\theta{1}}J(\theta{0},\theta{1})$$\theta{0}:=temp0$$\theta{1}:=temp1$ About parameter alphaWe should adjust our parameter α to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong. Gradient Descent For Linear RegressionThe following is a derivation of $\frac{\partial}{\partial\theta_{j}}J(\theta)$ for example \begin{align} \frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1}) & = \frac{\partial}{\partial\theta_{j}} \frac{1}{2m} \sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})^{2} \\\ & = \frac{\partial}{\partial\theta_{j}} \frac{1}{2m} \sum\limits_{i=1}^{m}(\theta_{0}+\theta_{1}x_{i}-y_{i})^{2} \end{align}For $\theta_{0}$ \frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1})=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})For $\theta_{1}$ \frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1})=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i})\cdot x_{i}Finally,we can substitute our actual cost function and our actual hypothesis function and modify the equation to : \theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x_{i})-y_{i}) \\ \theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}((h_{\theta}(x_{i})-y_{i})x_{i})This method looks at every example in the entire training set on every step, and is called batch gradient descent. Multiple FeaturesNotation: \begin{align*} x_j^{(i)} &= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}& = \text{the input (features) of the }i^{th}\text{ training example} \newline m &= \text{the number of training examples} \newline n &= \text{the number of features} \end{align*}The multivariable form of the hypothesis function accommodating these multiple features is as follows: h_{\theta} (x) = \theta_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + \theta_{3} x_{3} + \cdots + \theta_{n} x_{n}Define: $x_0=1$ h_{\theta} (x) = \theta_{0} x_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + \theta_{3} x_{3} + \cdots + \theta_{n} x_{n}Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as: \begin{align*} h_\theta(x) =\begin{bmatrix}\theta_{0} \hspace{2em} \theta_{1} \hspace{2em} \cdots \hspace{2em} \theta_{n}\end{bmatrix}\begin{bmatrix}x_{0} \newline x_{1} \newline \vdots \newline x_{n}\end{bmatrix} = \theta^{T}x \end{align*}Gradient Descent when $n&gt;1$ \begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*}Gradient Descent TricksFeature ScalingWe can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. Ideally: -1\leq x_{(i)}\leq 1or -0.5\leq x_{(i)}\leq 0.5Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.To implement both of these techniques, adjust your input values as shown in this formula: x_i := \dfrac{x_i - \mu_i}{s_i}Where $\mu_i$ is the average of all the values for feature $(i)$ and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation. For example, if $x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, x_i := \dfrac{price-1000}{1900}Learning RateDebugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(θ)$ over the number of iterations of gradient descent. If $J(θ)$ ever increases, then you probably need to decrease $\alpha$. Automatic convergence test. Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10−3. However in practice it’s difficult to choose this threshold value. It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration. Usually,choose a set of $\alpha$ for test,like:0.001 , 0.003 , 0.01 , 0.03 , … 1 For these different values of $\alpha$ are just plot $J(\theta)$ as a function of number of iterations, and then pick the value of $\alpha$ that seems to be causing $J(\theta)$ to decrease rapidly. To summarize: If $\alpha$ is too small: slow convergence. If $\alpha$ is too large: ￼may not decrease on every iteration and thus may not converge. Features and Polynomial RegressionWe can combine multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking x1⋅x2. For example:$area=width * height$ Polynomial RegressionOur hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). If you choose your features this way then feature scaling becomes very important. Normal Equation \theta = (X^T X)^{-1}X^T yNormal equation formula derivation \begin{align} Y &= X \theta \\\ X^T Y&=X^T X \theta \\\ (X^T X)^{-1}X^T Y &=\theta \end{align}Comparison Gradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate $O(kn^2)$ $O(n^3)$, need to calculate inverse of $X^T X$ Works well when $n$ is large Slow if $n$ is very large 内容参考Machine Learning by Andrew Ng掰开揉碎推导Normal Equation]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>Personal notes</tag>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learining--Introduction]]></title>
    <url>%2F2018%2F01%2F31%2FMachine_Learning_episode_01%2F</url>
    <content type="text"><![CDATA[机器学习的目标是使得学得的模型能很好地适用于“新样本”，这个能力，称为“泛化”(generalization)能力。 归纳(induction)：从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律，“从样例中学习”显然就是个归纳的过程； 演绎(deduction)：从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体情况，例如从一组公理和推理规则推导出与之相恰的定理； 关于机器学习有两个广为人知的定义，第一个来自Arthur Samuel，一个较老且不太正式的定义：The field of study that gives computers the ability to learn without being explicitly programmed. 还有一种则来自于Carnegie Mellon的教授Tom Mitchell： A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. 以下棋为例：E=下很多局棋所得的经验T=下棋这个挑战P=这个程序下赢棋的概率 根据训练数据是否拥有标记信息，学习任务可大致分为两大类： Supervised learning—监督学习(“right answer” given) Unsupervised learning—非监督学习(no feedback) Supervised LearningIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. RegressionWe are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function ClassificationWe are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories Unsupervised LearningUnsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. ClusteringWe can derive this structure by clustering the data based on relationships among the variables in the data.With unsupervised learning there is no feedback based on the prediction results. 归纳偏好当需要找到一条穿过所有训练样本的曲线时，存在着许多条曲线与其一致，所有这里的学习算法必须有某种偏好，才能找到它认为“正确”的模型。 “奥卡姆剃刀”(Occam’s razor)是一种常用的，自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”，在这里就是选曲线中最平滑的那个。 根据“没有免费的午餐”定理(No Free Lunch Theorem)，无论学习算法$Q{a}$多聪明，学习算法$Q{b}$多笨拙，它们的期望值是相同的，也就是说脱离了具体问题谈“什么学习算法更好”是毫无意义的。 內容參考Machine Learning by Andrew Ng机器学习]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>Personal notes</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然常数的相关了解]]></title>
    <url>%2F2018%2F01%2F07%2FEulers-Number%2F</url>
    <content type="text"><![CDATA[在学习线性回归的时候发现自己并不能理解正态分布的方程式，尤其对自然常数exp感到陌生，遂查阅相关资料，了解了一下关于这个自然常数的前生今世。自然常数exp是一个数学常量，是个无限不循环小数，约为2.71828，它的定义为： \exp=\sum_{n=0}^{+ \infty} \frac{1}{n!}=\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+\frac{1}{4!}\cdots或者是： \exp=\lim_{n \rightarrow + \infty}(1+\frac{1}{n})^{n}它有时候被称为欧拉数(Euler’s number),因为十八世纪初,数学大师Leonard Euler发现了这个自然常数。当时，Euler试图解决Jacob Bernoulli于半世纪前提出的银行利率问题： 假定你在银行存了一笔钱，银行每年给100%的利率，一年后，你会得到: (1+100\%)^1 = 2 现在假设银行每六个月结算一次利息，但只能提供利率的一半，即50%。在这种情况下，一年后的收益为: (1+50\%)^2=2.25 而假设银行每四个月提供33.3%复利息: (1+33.3\%)^{3} \approx 2.37 甚至银行愿意每天都提供利息的话，一年的利息: (1+\frac{1}{365})^{365} \approx 2.71456748202如果银行丧心病狂到愿意按秒给利息了，1年31536000秒，利滚利的余额: (1+\frac{1}{31536000})^{31536000} \approx 2.7182817813到头来才发现，也没多挣几个钱，而这个数值却越来越接近exp，所以exp就像另外一个数学常量π一样，假定直径长为1，通过割圆术分割出多边形的边数越多，算出来的周长也越接近于π。 微积分中的exp微积分中的exp更为神奇，对下式f(x)进行微分，无论进行几次结果还仍然为f(x)，难怪数学系学生会用exp比喻坚定不移的爱情！ f(x)=\exp^{x}就是说f(x)这条曲线上任意点的斜率等于当前f(x)的值，如下图所示： 就是说f(x)是一个无法降维的指数函数，因为它的导数等于其自身。 相关参考Wikipedia:e (mathematical constant))数学里的e为什么叫做自然底数？是不是自然界里什么东西恰好是e？An Intuitive Guide To Exponential Functions &amp; e《天才引导的历程》]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>mathematical constant</tag>
        <tag>Euler&#39;s Number</tag>
        <tag>exp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTex 数学公式]]></title>
    <url>%2F2018%2F01%2F06%2FLaTex-Mathematics%2F</url>
    <content type="text"><![CDATA[最近开始用hexo写博客，markdown语言采用MathJax引擎来显示数学公式，采用的是LaTex的语法，不仅方便使用，且渲染出来的公式比Word上好看多了。本文是鄙人在学习使用中边学边整理而成，并会不断更新，主要内容参考于文章下方的参考网页。 hexo渲染问题Hexo默认使用”hexo-renderer-marked”引擎渲染网页，有时候会莫名其妙有些公式不能正确渲染，网上最常见的解决方案是卸载hexo-renderer-marked，改用hexo-renderer-pandoc，通过下列指令完成：12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save 通用语法LaTeX公式有两种调用形式，一种是行内显示公式用$$...$$显示，还有一种是单独一行显示，用$$$...$$$，效果如下：123公式1：$Mathematics1$公式2：$$ Mathematics2 $$ 显示效果： 公式1：$Mathematics1$ 公式2：Mathematics2 具体各种用法均总结于下列表格中： 各种结构体 constructions LaTex Code constructions LaTex Code \widetilde{abc} $\widetilde{abc}$ \widehat{abc} $\widehat{abc}$ \overleftarrow{abc} $\overleftarrow{abc}$ \overrightarrow{abc} $\overrightarrow{abc}$ \overline{abc} $\overline{abc}$ \underline{abc} $\underline{abc}$ \overbrace{abc} $\overbrace{abc}$ \underbrace{abc} $\underbrace{abc}$ \sqrt{abc} $\sqrt{abc}$ \sqrt[n]{abc} $\sqrt[n]{abc}$ f’ $f’$ \frac{abc}{xyz} $\frac{abc}{xyz}$ 希腊符号(小) Greek letter LaTex Code Greek letter LaTex Code \alpha $\alpha$ o $ o$ \beta $\beta$ \pi $\pi$ \gamma $\gamma$ \varpi $\varpi$ \delta $\delta$ \rho $\rho$ \epsilon $\epsilon$ \varrho $\varrho$ \varepsilon $\varepsilon$ \sigma $\sigma$ \zeta $\zeta$ \varsigma $\varsigma$ \eta $\eta$ \tau $\tau$ \theta $\theta$ \upsilon $\upsilon$ \vartheta $\vartheta$ \phi $\phi$ \kappa $\kappa$ \varphi $\varphi$ \lambda $\lambda$ \chi $\chi$ \mu $\mu$ \psi $\psi$ \nu $\nu$ \omega $\omega$ \xi $\xi$ 希腊符号(大) Greek letter LaTex Code Greek letter LaTex Code \Gamma $\Gamma$ \Sigma $\Sigma$ \Delta $\Delta$ \Upsilon $\Upsilon$ \Theta $\Theta$ \Phi $\Phi$ \Lambda $\Lambda$ \Psi $\Psi$ \Xi $\Xi$ \Omega $\Omega$ \Pi $\Pi$ 常用二元运算符 Symbols LaTex Code Symbols LaTex Code \pm $\pm$ \mp $\mp$ \times $\times$ \div $\div$ \ast $\ast$ \cdot $\cdot$ \circ $\circ$ \bullet $\bullet$ + $ +$ - $ -$ \cap $\cap$ \cup $\cup$ \vee $\vee$ \wedge $\wedge$ 常用关系运算符 Symbols LaTex Code Symbols LaTex Code &lt; $ &lt;$ &gt; $ &gt;$ \ll $\ll$ \gg $\gg$ \leq $\leq$ \geq $\geq$ = $ =$ \neq $\neq$ \approx $\approx$ \doteq $\doteq$ \subset $\subset$ \supset $\supset$ \subseteq $\subseteq$ \supseteq $\supseteq$ \in $\in$ \in $\in$ \mid $\mid$ \parallel $\parallel$ 大小可变符号 Symbols LaTex Code Symbols LaTex Code \sum $\sum$ $$ \prod $\prod$ \coprod $\coprod$ \bigcap $\bigcap$ \bigcup $\bigcup$ \bigvee $\bigvee$ \bigwedge $\bigwedge$ \int $\int$ \oint $\oint$ \iint $\iint$ \iiint $\iiint$ Log-like 符号 Symbols LaTex Code Symbols LaTex Code \arccos $\arccos$ \cos $\cos$ \arcsin $\arcsin$ \sin $\sin$ \arctan $\arctan$ \tan $\tan$ \log $\log$ \ln $\ln$ \exp $\exp$ \max $\max$ \lim $\lim$ 数学模式的口音 Symbols LaTex Code Symbols LaTex Code \hat{a} $\hat{a}$ \check{a} $\check{a}$ \acute{a} $\acute{a}$ \grave{a} $\grave{a}$ \bar{a} $\bar{a}$ \vec{a} $\vec{a}$ \dot{a} $\dot{a}$ \ddot{a} $\ddot{a}$ \breve{a} $\breve{a}$ \tilde{a} $\tilde{a}$ 各种符号 Symbols LaTex Code Symbols LaTex Code \ldotp $\ldotp$ \cdotp $\cdotp$ \ldots $\ldots$ \cdots $\cdots$ \colon $\colon$ \neg $\neg$ \forall $\forall$ \exists $\exists$ \partial $\partial$ \infty $\infty$ \sharp $\sharp$ 箭头符号 Symbols LaTex Code Symbols LaTex Code \leftarrow $\leftarrow$ \rightarrow $\rightarrow$ \longleftarrow $\longleftarrow$ \longrightarrow $\longrightarrow$ \Leftarrow $\Leftarrow$ \Rightarrow $\Rightarrow$ \Longleftarrow $\Longleftarrow$ \Longrightarrow $\Longrightarrow$ \leftrightarrow $\leftrightarrow$ \longleftrightarrow $\longleftrightarrow$ \Leftrightarrow $\Leftrightarrow$ \Longleftrightarrow $\Longleftrightarrow$ \uparrow $\uparrow$ \downarrow $\downarrow$ \Uparrow $\Uparrow$ \Downarrow $\Downarrow$ 多行公式显示方法：多个方程式 方程式间使用\\来进行换行，实际使用中，需要进行转义，即使用\\\来进行换行，效果如下所示：1234$$f(x)=\exp^&#123;x&#125;\\\f(y)=\exp^&#123;y&#125;$$ 显示效果： f(x)=\exp^{x}\\\ f(y)=\exp^{y}数学运算拆解的式子于等号对齐使用了align指令，每个=前都加上&amp;,使用\\\进行换行123456789$$\begin&#123;align&#125;\sqrt&#123;37&#125; &amp; = \sqrt&#123;\frac&#123;73^2-1&#125;&#123;12^2&#125;&#125; \\\ &amp; = \sqrt&#123;\frac&#123;73^2&#125;&#123;12^2&#125;\cdot\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\ &amp; = \sqrt&#123;\frac&#123;73^2&#125;&#123;12^2&#125;&#125;\sqrt&#123;\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\ &amp; = \frac&#123;73&#125;&#123;12&#125;\sqrt&#123;1 - \frac&#123;1&#125;&#123;73^2&#125;&#125; \\\ &amp; \approx \frac&#123;73&#125;&#123;12&#125;\left(1 - \frac&#123;1&#125;&#123;2\cdot73^2&#125;\right)\end&#123;align&#125;$$ 显示结果 \begin{align} \sqrt{37} & = \sqrt{\frac{73^2-1}{12^2}} \\\ & = \sqrt{\frac{73^2}{12^2}\cdot\frac{73^2-1}{73^2}} \\\ & = \sqrt{\frac{73^2}{12^2}}\sqrt{\frac{73^2-1}{73^2}} \\\ & = \frac{73}{12}\sqrt{1 - \frac{1}{73^2}} \\\ & \approx \frac{73}{12}\left(1 - \frac{1}{2\cdot73^2}\right) \end{align}矩阵与对齐使用了array指令，同样使用\\\进行换行1234567891011$$\mathbf&#123;X&#125; = \left( \begin&#123;array&#125; &#123;ccc&#125; x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\ x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\ \vdots &amp; \vdots &amp; \ddots \end&#123;array&#125; \right)$$ 显示结果 \mathbf{X} = \left( \begin{array} {ccc} x\_{11} & x\_{12} & \ldots \\\ x\_{21} & x\_{22} & \ldots \\\ \vdots & \vdots & \ddots \end{array} \right)括号使用与对齐在LaTeX中使用\left和\right成对出现来表示一对括号，如果不使用该指令，直接用()也是可以的，只是括号大小不会自动渲染。1$$\left(1+\left(\frac &#123;1&#125;&#123;\exp^2&#125;\right)\right) \qquad 1+(\frac &#123;1&#125;&#123;\exp^2&#125;)$$ 上面代码渲染出的两种括号大小如下所示： 1+\left(\frac {1}{\exp^2}\right) \qquad 1+(\frac {1}{\exp^2})另，当因括号层次需要调节括号大小时，使用\big,\Big,\bigg12$$\big(\Big(\bigg(\Bigg($$$$\big[\Big[\bigg[\Bigg[$$ 效果如下所示： \big(\Big(\bigg(\Bigg(\big[\Big[\bigg[\Bigg[上下限位置问题这里需要使用\limits指令，即在$\sum\prod\cdots$这类可以设定上下限的指令后面接上\limits，具体效果如下所示：未使用\limits指令：1R^&#123;2&#125;=1-\frac&#123;\sum_&#123;i=1&#125;^&#123;m&#125;(\hat&#123;y_&#123;i&#125;&#125;-y_&#123;i&#125;)&#125;&#123;\sum_&#123;i=1&#125;^&#123;m&#125;(y_&#123;i&#125;-\bar&#123;y_&#123;i&#125;&#125;)&#125; R^{2}=1- \frac{ \sum_{i=1}^{m}(\hat{y_{i}}-y_{i})}{\sum_{i=1}^{m}(y_{i}-\bar{y_{i}})}使用了\limits指令的：1R^&#123;2&#125;=1-\frac&#123;\sum\limits_&#123;i=1&#125;^&#123;m&#125;(\hat&#123;y_&#123;i&#125;&#125;-y_&#123;i&#125;)&#125;&#123;\sum\limits_&#123;i=1&#125;^&#123;m&#125;(y_&#123;i&#125;-\bar&#123;y_&#123;i&#125;&#125;)&#125; R^{2}=1-\frac{\sum\limits_{i=1}^{m}(\hat{y_{i}}-y_{i})}{\sum\limits_{i=1}^{m}(y_{i}-\bar{y_{i}})}输入文字有时候需要在公式中加入文字表述，使用到的是\text{}指令1\text&#123;爱因斯坦质能方程&#125;:E=mc^2 显示效果： \text{爱因斯坦质能方程}:E=mc^2参考网页：MathJax basic tutorial and quick reference维基教科书:LaTeX/数学公式LaTeX Math Symbols怎样输入完美的TeX公式]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>LaTex</tag>
        <tag>markdown</tag>
        <tag>mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaCross01]]></title>
    <url>%2F2018%2F01%2F03%2FJavacross-01%2F</url>
    <content type="text"><![CDATA[本文内容对应《Head First Java》一书中第一章的字谜游戏，相关单词内容可参考网址：studystack 横排提示 4.Command-line invoker6.Back again?8.Can’t go both ways9.Acronym for your laptop’s power12.number variable type13.Acronym for a chip14.Say something18.Quite a crew of characters19.Announce a new class or method21.What’s a prompt good for? 竖排提示 1.Not an integer (or _ your boat)2.Come back empty-handed3.Open house5.‘Things’ holders7.Until attitudes improve10.Source code consumer11.Can’t pin it down13.Dept. of LAN jockeys15.Shocking modifier16.Just gotta have one17.How to get things done20.Bytecode consumer 答案见下 index hint word index hint word 1 Not an integer (or _ your boat) float 12 number variable type int 2 Come back empty-handed void 13 Acronym for a chip IC 3 Open house public 14 Say something SystemOutPrint 4 Command-line invoker java 15 Shocking modifier Static 5 ‘Things’ holders arrays 16 Just gotta have one main 6 Back again? loop 17 How to get things done method 7 Until attitudes improve while 18 Quite a crew of characters string 8 Can’t go both ways branch 19 Announce a new class or method declare 9 Acronym for your laptop’s power DC 20 Bytecode consumer JVM 10 Source code consumer compiler 21 What’s a prompt good for? command 11 Can’t pin it down variable 书中答案截图如下所示： Tipsindex:16 Just gotta have one 无论程序有多大（也可以说不管有多少类），一定都会有一个main()来作为程序的起点。 Head First Java]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>head_first</tag>
        <tag>java_cross</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intro]]></title>
    <url>%2F2017%2F12%2F20%2FIntro%2F</url>
    <content type="text"><![CDATA[例行公事，秀个恩爱先]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>selfie</tag>
      </tags>
  </entry>
</search>
